{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/logo-di.jpg\" style=\"height: 4rem; margin-top: 1.1rem; float: left;\">\n",
    "<img src=\"img/logo-unimi.jpg\" style=\"height: 3.5rem; float: left; margin-right: 2em;\">\n",
    "\n",
    "<h3 syle=\"float: left;\">Master in Computer Science</h3>\n",
    "\n",
    "\n",
    "<p style=\"clear: both;\" />\n",
    "<h2>Algorithms for massive datasets</h2>\n",
    "\n",
    "<h1>Regression and logistic regression at big scale</h1>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<address>Dario Malchiodi<br/>\n",
    "malchiodi@di.unimi.it</address>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# It's (also) a machine learning problem\n",
    "\n",
    "* Data: a set $\\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)}) \\}$ of associations between *objects* and *labels*\n",
    "* Goal: find a mapping from objects to labels\n",
    "  - describing observed data within a reasonable approximation level\n",
    "  - generalizing to unseen observations\n",
    "* Technically, a supervised learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "> Pyro: That's a dorky looking helmet. What's it for?\n",
    ">\n",
    "> Magneto: This «dorky looking helmet» is the only thing that's going to protect me from the REAL bad guys.\n",
    "\n",
    "A good notation is like Magneto's helmet\n",
    "\n",
    "* $x^{(j)}$ denotes $j$-th object/vector in a series ($y^{(j)}$ the $j$-th label)\n",
    "* (to avoid confusion with exponentiation)\n",
    "* $x_i$ denotes $i$-th component of vector $x$\n",
    "* (mix and match: $x^{(j)}_i$ denotes $i$-th component of $j$-th vector)\n",
    "\n",
    "Will try to be consistent in using $i$ as component index and $j$ as object/vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "* generic object: $x \\in \\mathbb R^d$ for fixed $d \\in \\mathbb N$\n",
    "* generic label: $y \\in \\mathbb R$\n",
    "* approximation of label: $\\hat y \\in \\mathbb R$\n",
    "\n",
    "Assume a *linear* mapping between objects and labels\n",
    "\n",
    "$$ \\hat y = w \\cdot x = \\sum_{i=1}^d w_i x_i $$\n",
    "\n",
    "* $\\hat y$ is an approximation (or a prediction) for $y$\n",
    "* Our problem lies in finding $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Affine mapping integration\n",
    "\n",
    "* Adding a threshold/offset in the mapping may help a lot\n",
    "* Technically, this brings us to an affine mapping\n",
    "* Practically, just pretend you have an additional dimension and set its component to 1\n",
    "\n",
    "$$ x = (x_1, \\dots, x_d) \\rightarrow x = (1, x_1, \\dots, x_d) $$\n",
    "\n",
    "$$ \\hat y = w \\cdot x = \\sum_{i=0}^d w_i x_i = w_0 + w_1 x_1 + \\dots + w_d x_d $$\n",
    "\n",
    "Nothing changed in our problem (still in search for $w$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uh, rather simple?\n",
    "\n",
    "* A feature!\n",
    "* An option not to be underestimated\n",
    "* Complexity injectable through feature extraction:\n",
    "  - augment object vectors adding product of components\n",
    "  - for instance, pairs of components capture covariance\n",
    "  - extendible to higher order moments\n",
    "\n",
    "For instance: $x = (x_1, x_2) \\rightarrow \\Phi(x) = (x_1^2, x_1 x_2, x_2 x_1, x_2^2)$\n",
    "\n",
    "Let's be clever: $\\Phi'(x) = (x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$, because\n",
    "\n",
    "\\begin{align}\n",
    "\\Phi'(a) \\cdot \\Phi'(b) &= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 + a_2^2 b_2^2 \\\\\n",
    "                        &= \\Phi(a) \\cdot \\Phi(b)\n",
    "\\end{align}\n",
    "\n",
    "Besides,\n",
    "\n",
    "$$ \\Phi'(a) \\cdot \\Phi'(b) = (a_1 b_1 + a_2 b_2)^2 = (a \\cdot b)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do I find $w$?\n",
    "\n",
    "* Pretend we have a candidate $w$\n",
    "* Let's measure how $w$ is good at prediction:\n",
    "  - $y$ is my label\n",
    "  - $\\hat y$ is the prediction\n",
    "  - need a loss function: squared error $(\\hat y - y)^2$\n",
    "* Let's cumulate errors on all observations:\n",
    "\n",
    "$$ \\ell(w) = \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "Find $w$ minimizing cumulated loss:\n",
    "\n",
    "\\begin{align}\n",
    "w &= \\arg \\min_w \\ell(w) \\\\\n",
    "  &= \\arg \\min_w \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 \\\\\n",
    "  &= \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "- gather objects in the $n \\times d$ matrix $X$\n",
    "- gather labels in vector $y \\in \\mathbb R^n$\n",
    "\n",
    "$$ w = \\arg \\min_w || X w - y ||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "It's a convex problem, just nullify first derivatives:\n",
    "\n",
    "$$ \\frac{\\partial \\ell}{\\partial w} = 2 X^T (X w - y)  = 0 $$\n",
    "\n",
    "This brings to\n",
    "\n",
    "\\begin{align}\n",
    "X^T X w - X^T y &= 0 \\\\\n",
    "w = \\left( X^T X \\right)^{-1} X^T y\n",
    "\\end{align}\n",
    "\n",
    "(remember: $X$ is $n \\times d$, $X^T$ is $d \\times n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Remember generalization?\n",
    "\n",
    "* Real-world data is *dirty*\n",
    "* Aiming at the smallest loss could lead to *overfitting*\n",
    "* Occam's razor: find the right balance between model complexity and error\n",
    "* For instance: *Ridge regression*\n",
    "\n",
    "$$ w = \\arg \\min_w || X w - y ||_2^2 + \\lambda || w ||_2^2 $$\n",
    "\n",
    "* Closed form solution\n",
    "\n",
    "$$ w = \\left( X^T X + \\lambda I_d \\right)^{-1} X^T y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wait: $\\lambda$?\n",
    "\n",
    "* Hyper-parameter to be tuned\n",
    "* How can it be selected?\n",
    "* And what about assessing the learnt model capabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use data against overfitting\n",
    "\n",
    "* Split observations in three sets:\n",
    "  - *Training set*, used to train models (in our case: finding out $w$)\n",
    "  - *Validation set*, used for model selection (in our case: tuning $\\lambda$)\n",
    "  - *Test set*, used to assess the machine learning output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessment\n",
    "\n",
    "* Fix an error measure, typically MSE\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "* or $ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning pipeline\n",
    "\n",
    "1. Fix a discretization $\\{ \\lambda_1, \\dots, \\lambda_o \\}$ of the parameter space\n",
    "2. For each discretized value $\\lambda_k$:\n",
    "   - run learning algorithm using $\\lambda = \\lambda_k$ and training set\n",
    "   - assess learnt model computing (R)MSE on validation set\n",
    "3. Run learning algorithm using $\\lambda = \\lambda^{\\text{opt}}$ (with $\\lambda^{\\text{opt}}$ corresponding to the lowest (R)MSE) and training$+$validation set\n",
    "4. Assess overall learning process computing (R)MSE on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational complexity for linear regression\n",
    "\n",
    "Remember that $w = \\left( X^T X \\right)^{-1} X^T y$\n",
    "\n",
    "* Time complexity: $\\mathrm O(n d^2 + d^3)$ basic operations\n",
    "  - $d^3$ for matrix inversion ($X$ is $n \\times d$, $X^T X$ is $d \\times d$)\n",
    "  - $n d^2$ for matrix multiplication\n",
    "* Space complexity: $\\mathrm O(n d + d^2)$ floats\n",
    "  - $d^2$ for $X^T X$ and its inverse\n",
    "  - $n d$ for $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big-scale regression\n",
    "\n",
    "Two scenarios:\n",
    "\n",
    "- big $n$, small $d$,\n",
    "- big $n$, big $d$\n",
    "- (what about small $n$ and big $d$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $n$, small $d$\n",
    "\n",
    "* Time complexity $\\mathrm O(n d^2 + d^3)$\n",
    "  - $\\mathrm O(d^3)$ for matrix inversion is acceptable\n",
    "* Space complexity $\\mathrm O(nd + d^2)$\n",
    "  - $\\mathrm O(d^2)$ for $X^T X$ storage is acceptable\n",
    "* Instead, computation of $X^T X$ and storage of $X$ are bottlenecks\n",
    "\n",
    "Solution\n",
    "\n",
    "* Distribute storage of $X$ across cluster nodes\n",
    "* Express $X^T X$ as a sum of outer products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix product through outer products\n",
    "\n",
    "Let $A$ be a $n \\times d$ matrix and $B$ be a $d \\times m$ matrix:\n",
    "\n",
    "$$ (AB)_{ij} = \\sum_{k=1}^d a_{ik} b_{kj} $$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$ AB = \\sum_{k=1}^d P_k $$\n",
    "\n",
    "where $(P_k)_{ij} = a_{ik} b_{kj}$, which means that $P_k = A_k \\oplus B^k$ (outer product of $k$-th column of $A$ and $k$-th row of $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An example\n",
    "\n",
    "\\begin{align}\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "9 & 3 & 5 \\\\\n",
    "4 & 1 & 2 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & -5 \\\\\n",
    "2 & 3\n",
    "\\end{array}\n",
    "\\right]\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "9 \\\\ 4\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "1 \\\\ 2\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "3 \\\\ 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "3 \\\\ -5\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "5 \\\\ 2\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "2 \\\\ 3\n",
    "\\end{array}\n",
    "\\right]\\\\\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "9 & 18 \\\\\n",
    "4 & 8\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "9 & -15 \\\\\n",
    "3 & -5\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "10 & 15 \\\\\n",
    "4 & 6\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "28 & 18 \\\\\n",
    "11 & 9 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed computation of $X^T X$\n",
    "\n",
    "* Compute $X^T X$ as $\\sum_{k=1}^d X^T_k \\oplus X^k = \\sum_{k=1}^d x_k \\cdot x_k$\n",
    "  - requires local storage of $\\mathrm O(d^2)$, local computation of $\\mathrm O(d^2)$\n",
    "* Compute $(X^T X)^{-1}$ summing local results and inverting\n",
    "  - requires local storage of $\\mathrm O(d^2)$, local computation of $\\mathrm O(d^3)$\n",
    "\n",
    "``train.map(computer_outer_product)\n",
    "     .reduce(sum_and_invert)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $d$, big $n$\n",
    "\n",
    "* Time complexity $\\mathrm O(n d^2 + d^3)$\n",
    "  - also matrix inversion is a bottleneck\n",
    "* Space complexity $\\mathrm O(nd + d^2)$\n",
    "  - also storage of $X^T X$ is a bottleneck\n",
    "* And of course previous bottlenecks are still there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $d$, big $n$\n",
    "\n",
    "Solution\n",
    "\n",
    "* A different approach to linear regression\n",
    "  - Rule of thumb: computation and storage should be linear in $d$ and $n$\n",
    "* Exploit sparsity\n",
    "  - explicit\n",
    "  - implicit\n",
    "* Use a different algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "Assume $f: \\mathbb R \\mapsto \\mathbb R$\n",
    "\n",
    "1. Choose $x_0 \\in \\mathbb R$, set $i=0$\n",
    "2. Repeat until convergence\n",
    "   - $x_{i+1} = x_i - f'(x_i)$\n",
    "   - $i = i+1$\n",
    "   \n",
    "This algorithm converges to a *local* minimum $x^*$ for $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "If $f : \\mathbb R^d \\mapsto \\mathbb R$\n",
    "\n",
    "1. Choose $x_0 \\in \\mathbb R^d$, set $i=0$\n",
    "2. Repeat until convergence\n",
    "   - $x_{i+1} = x_i - \\nabla f(x_i)$\n",
    "   - $i = i+1$\n",
    "\n",
    "Where $\\nabla f(x) \\in \\mathbb R^d$ with $\\nabla f(x)_i = \\partial f(x) / \\partial x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "Critical issues\n",
    "\n",
    "* Choosing initial point\n",
    "* Setting step size (*learning rate*)\n",
    "\n",
    "$x_{i+1} = x_i - \\eta f'(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent for linear regression\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "Thus $f(w) = \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_i}(w) = \\sum_{j=1}^n 2 \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w}(w) = \\sum_{j=1}^n 2 \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}\n",
    "$$\n",
    "\n",
    "Local minimization through gradient descent\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\sum_{j=1}^n \\left(w_t \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convexity\n",
    "\n",
    "Linear regression is a convex problem, so gradient descent is OK\n",
    "\n",
    "# Dynamic learning rate\n",
    "\n",
    "$$\\eta \\mapsto \\eta_t = \\frac{\\eta}{n \\sqrt{t}}$$\n",
    "\n",
    "* Big steps at the beginning of iteration\n",
    "* Small steps as we reach convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelization of gradient descent\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta_t \\sum_{j=1}^n \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)} $$\n",
    "\n",
    "* Send $w_t$ to all workers\n",
    "* Compute summands in parallel\n",
    "* Now each worker stores $w_t$ and $x^{(j)}$ (space complexity is $\\mathrm O(d)$)\n",
    "* Computation is $\\mathrm O(d)$, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification problem\n",
    "\n",
    "Not that different from the ML framing of regression\n",
    "\n",
    "* Data: a set $\\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)}) \\}$ of associations between *objects* and *labels*\n",
    "* Goal: find a mapping from objects to labels\n",
    "  - describing observed data within a reasonable approximation level\n",
    "  - generalizing to unseen observations\n",
    "* Technically, a supervised learning problem\n",
    "\n",
    "Now, labels belong to a *discrete* set\n",
    "\n",
    "* positive/negative (binary classification, we'll stick on this)\n",
    "* multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear classification\n",
    "\n",
    "* Use something similar to regression in order to find two half-spaces for objects\n",
    "* Classify according to the half-space where objects belong\n",
    "\n",
    "$$ \\hat y = \\mathrm{sign}(w x) $$\n",
    "\n",
    "* Note that $y \\in \\{ -1, 1 \\}$ in order for $\\hat y$ be reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating predictions\n",
    "\n",
    "* In regression: $L_2$ loss $\\left( \\hat y - y \\right)^2$\n",
    "* In binary classification: $0/1$ loss\n",
    "  - null penalty in case of correct classification\n",
    "  - unitary penalty in case of misclassification\n",
    "* Let $z = \\hat y y$\n",
    "$$\n",
    "\\ell_{0/1}(z) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if } z < 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the classifier\n",
    "\n",
    "* In regression $w = \\arg \\min_w \\ell(w)$\n",
    "* In our case\n",
    "\n",
    "\\begin{align}\n",
    "w &= \\arg \\min \\ell_{0/1}(w) \\\\\n",
    "  &= \\arg \\min \\sum_{j=1}^n \\ell_{0/1}\\left(y^{(j)} w x^{(j)}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A problem with convexity\n",
    "\n",
    "$0/1$ loss is not convex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwx9SWqIoS9JDTH0Jakh/wcZJNAew0KRUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117df8710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def zero_one_loss(z):\n",
    "    return 0 if z >= 0 else 1\n",
    "\n",
    "n = 20\n",
    "z = np.arange(-5, 10, .1)\n",
    "g = plt.plot(z, map(zero_one_loss, z))\n",
    "plt.ylim([-1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximate loss\n",
    "\n",
    "* Need for convexity\n",
    "* Between various possibilities, *log loss*\n",
    "\n",
    "$$ \\ell_{\\log}(z) = \\log \\left( 1 + \\mathrm e^{-z} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117df8110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_loss(z):\n",
    "    return np.log(1 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-3, 10, .5)\n",
    "g = plt.plot(z, map(log_loss, z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Result: logistic regression\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) $$\n",
    "\n",
    "Thus we are optimizing\n",
    "\n",
    "$$ f(w) = \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) $$\n",
    "\n",
    "* Optimization through gradient descent\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta \\nabla f(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computing derivatives\n",
    "\n",
    "As\n",
    "\n",
    "$$ \\frac{\\partial \\ell_{\\log(z)}}{\\partial z} =\n",
    "   \\frac{-\\mathrm e^{-z}}{1 + \\mathrm e^{-z}} =\n",
    "   -\\left( 1 - \\frac{1}{1 + \\mathrm e^{-z}} \\right) $$\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial w_k} &=\n",
    "\\sum_{j=1}^n \\left[ 1- \\frac{1}{1+ \\mathrm e^{ -y^{(j)} w x^{(j)} } } \\right] \\left( -y^{(j)} x^{(j)_k} \\right)\n",
    "\\end{align}\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial w} &=\n",
    "\\sum_{j=1}^n \\left( 1- \\frac{1}{1+ \\mathrm e^{ -y^{(j)} w x^{(j)} } } \\right) \\left( -y^{(j)} x^{(j)} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularized logistic regression\n",
    "\n",
    "As in ridge regression, add a regularization term\n",
    "\n",
    "$$ \\min_w \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) + \\lambda || w ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic interpretation\n",
    "\n",
    "* A step ahead, instead of predicting class for an object $x$...\n",
    "* ...estimate the probability of a class *given* the object\n",
    "\n",
    "$$\\mathrm P(Y=1 | X=x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic interpretation\n",
    "\n",
    "* Can't use linear regression: $\\mathrm P(Y=1 | X=x) \\neq wx$\n",
    "  - because probabilities belong to $[0, 1]$\n",
    "* Can't use sign: $\\mathrm P(Y=1 | X=x) \\neq \\mathrm{sign}(wx)$\n",
    "  - for same resaon of before\n",
    "* Can use logistic function: $\\mathrm P(Y=1 | X=x) = \\sigma(wx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic function\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\mathrm e^{-z}} $$\n",
    "\n",
    "kinda of smooth approximation of a step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUDNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114154fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-7, 7, .5)\n",
    "g = plt.plot(z, map(logistic, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predicting probabilities\n",
    "\n",
    "1. Use logistic regression to learn $w$\n",
    "2. Predict probabilities as $\\mathrm P(Y=1 | X=x) = \\sigma(wx)$\n",
    "\n",
    "# Classifying using probabilities\n",
    "\n",
    "Threshold probability: $x$ is positive if $\\mathrm P(Y=1 | X=x) > 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing threshold: ROC curves\n",
    "\n",
    "Two kind of errors:\n",
    "\n",
    "* false positives (FP): objects classified as positive when they are negative\n",
    "* false negatives (FN): objects classified as negative when they are positive\n",
    "\n",
    "$$ \\mathrm{TPR} = \\frac{TP}{TP+FN}, \\quad \\mathrm{FPR} = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing threshold: ROC curves\n",
    "\n",
    "* threshold = 0: everything is positive, FN=TN=0\n",
    "* threshold = 1: everything is negative, FP=TP=0\n",
    "\n",
    "![ROC curve](img/roc.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
